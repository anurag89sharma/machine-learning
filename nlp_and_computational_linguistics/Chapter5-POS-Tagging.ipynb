{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_0 = nlp(u'Mathieu and I went to the park.')\n",
    "sent_1 = nlp(u'If Clement was asked to take out the garbage, he would refuse.')\n",
    "sent_2 = nlp(u'Baptiste was in charge of the refuse treatment center.')\n",
    "sent_3 = nlp(u'Marie took out her rather suspicious and fishy cat to go fish for fish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mathieu', 'PROPN', 'NNP')\n",
      "('and', 'CCONJ', 'CC')\n",
      "('I', 'PRON', 'PRP')\n",
      "('went', 'VERB', 'VBD')\n",
      "('to', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('park', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_0:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('If', 'ADP', 'IN')\n",
      "('Clement', 'PROPN', 'NNP')\n",
      "('was', 'VERB', 'VBD')\n",
      "('asked', 'VERB', 'VBN')\n",
      "('to', 'PART', 'TO')\n",
      "('take', 'VERB', 'VB')\n",
      "('out', 'PART', 'RP')\n",
      "('the', 'DET', 'DT')\n",
      "('garbage', 'NOUN', 'NN')\n",
      "(',', 'PUNCT', ',')\n",
      "('he', 'PRON', 'PRP')\n",
      "('would', 'VERB', 'MD')\n",
      "('refuse', 'VERB', 'VB')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_1:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Baptiste', 'PROPN', 'NNP')\n",
      "('was', 'VERB', 'VBD')\n",
      "('in', 'ADP', 'IN')\n",
      "('charge', 'NOUN', 'NN')\n",
      "('of', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('refuse', 'ADJ', 'JJ')\n",
      "('treatment', 'NOUN', 'NN')\n",
      "('center', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_2:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marie', 'PROPN', 'NNP')\n",
      "('took', 'VERB', 'VBD')\n",
      "('out', 'PART', 'RP')\n",
      "('her', 'PRON', 'PRP')\n",
      "('rather', 'ADV', 'RB')\n",
      "('suspicious', 'ADJ', 'JJ')\n",
      "('and', 'CCONJ', 'CC')\n",
      "('fishy', 'ADJ', 'JJ')\n",
      "('cat', 'NOUN', 'NN')\n",
      "('to', 'PART', 'TO')\n",
      "('go', 'VERB', 'VB')\n",
      "('fish', 'NOUN', 'NN')\n",
      "('for', 'ADP', 'IN')\n",
      "('fish', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in sent_3:\n",
    "    print((token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your own POS-tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "TAG_MAP = {\n",
    "    'N': {'pos': 'NOUN'},\n",
    "    'V': {'pos': 'VERB'},\n",
    "    'J': {'pos': 'ADJ'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"I like green eggs\", {'tags': ['N', 'V', 'J', 'N']}),\n",
    "    (\"Eat blue ham\", {'tags': ['V', 'J', 'N']})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "\n",
    "def main(lang='en', output_dir=None, n_iter=25):\n",
    "    \"\"\"Create a new model, set up the pipeline and train the tagger. In order to\n",
    "    train the tagger with a custom tag map, we're creating a new Language\n",
    "    instance with a custom vocab.\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(lang)\n",
    "    # add the tagger to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    tagger = nlp.create_pipe('tagger')\n",
    "    # Add the tags. This needs to be done before you start training.\n",
    "    for tag, values in TAG_MAP.items():\n",
    "        tagger.add_label(tag, values)\n",
    "    nlp.add_pipe(tagger)\n",
    "    \n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            nlp.update([text], [annotations], sgd=optimizer, losses=losses)\n",
    "        print(losses)\n",
    "        \n",
    "    # test the trained model\n",
    "    test_text = \"I like blue eggs\"\n",
    "    doc = nlp(test_text)\n",
    "    print('Tags', [(t.text, t.tag_, t.pos_) for t in doc])\n",
    "    \n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the save model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc = nlp2(test_text)\n",
    "        print('Tags', [(t.text, t.tag_, t.pos_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "{'tagger': 0.5731045007705688}\n",
      "{'tagger': 0.5470398366451263}\n",
      "{'tagger': 0.4321519583463669}\n",
      "{'tagger': 0.26743578910827637}\n",
      "{'tagger': 0.12172483652830124}\n",
      "{'tagger': 0.03240921627730131}\n",
      "{'tagger': 0.0039986324263736606}\n",
      "{'tagger': 0.00036780487425858155}\n",
      "{'tagger': 4.438006453710841e-05}\n",
      "{'tagger': 8.323409701915807e-06}\n",
      "{'tagger': 1.8317383023713774e-06}\n",
      "{'tagger': 6.832339920492814e-07}\n",
      "{'tagger': 2.2298168289580644e-07}\n",
      "{'tagger': 9.801566491773883e-08}\n",
      "{'tagger': 5.375977885080374e-08}\n",
      "{'tagger': 2.9275055268840333e-08}\n",
      "{'tagger': 1.755438905348683e-08}\n",
      "{'tagger': 1.1347002004669093e-08}\n",
      "{'tagger': 7.573777383029778e-09}\n",
      "{'tagger': 5.780358192097879e-09}\n",
      "{'tagger': 4.297178834988813e-09}\n",
      "{'tagger': 3.438658691123919e-09}\n",
      "{'tagger': 2.829821377225983e-09}\n",
      "{'tagger': 2.4201857184991127e-09}\n",
      "{'tagger': 2.1093340407674077e-09}\n",
      "Tags [('I', 'N', 'NOUN'), ('like', 'V', 'VERB'), ('blue', 'J', 'ADJ'), ('eggs', 'N', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-tagging code examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom RAN swiftly and WALKED slowly\n"
     ]
    }
   ],
   "source": [
    "def make_verb_upper(text, pos):\n",
    "    return text.upper() if pos == \"VERB\" else text \n",
    "\n",
    "doc = nlp(u'Tom ran swiftly and walked slowly')\n",
    "text = ''.join(make_verb_upper(w.text_with_ws, w.pos_) for w in doc)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
