{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimension/.virtualenvs/data-science/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, (2, 3)] # petal length and petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "X_pred = per_clf.predict([[2, 0.5]])\n",
    "print(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mnist_path = os.path.join(\"/home/dimension/\", \"scikit_learn_data/mldata\", \"mnist-original.mat\")\n",
    "\n",
    "if not os.path.isfile(mnist_path):\n",
    "    # # download dataset from github.\n",
    "    mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "    response = urllib.request.urlopen(mnist_alternative_url)\n",
    "    with open(mnist_path, \"wb\") as f:\n",
    "        content = response.read()\n",
    "        f.write(content)\n",
    "\n",
    "mnist_raw = loadmat(mnist_path)\n",
    "mnist = {\n",
    "    \"data\": mnist_raw[\"data\"].T,\n",
    "    \"target\": mnist_raw[\"label\"][0],\n",
    "    \"COL_NAMES\": [\"label\", \"data\"],\n",
    "    \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an MLP with TensorFlow's High-Level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimension/.virtualenvs/data-science/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpow_e53kq\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpow_e53kq', '_log_step_count_steps': 100, '_num_worker_replicas': 0, '_save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2422f85a90>, '_tf_random_seed': None, '_train_distribute': None, '_master': '', '_keep_checkpoint_max': 5, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_evaluation_master': '', '_task_type': None, '_environment': 'local', '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_task_id': 0, '_session_config': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      "}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpow_e53kq/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 131.1536\n",
      "INFO:tensorflow:global_step/sec: 248.302\n",
      "INFO:tensorflow:step = 101, loss = 2.2520607 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.947\n",
      "INFO:tensorflow:step = 201, loss = 1.0158677 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.632\n",
      "INFO:tensorflow:step = 301, loss = 0.51389146 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.163\n",
      "INFO:tensorflow:step = 401, loss = 0.9190439 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.881\n",
      "INFO:tensorflow:step = 501, loss = 0.6657086 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.842\n",
      "INFO:tensorflow:step = 601, loss = 1.0600765 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.977\n",
      "INFO:tensorflow:step = 701, loss = 0.45323598 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.522\n",
      "INFO:tensorflow:step = 801, loss = 0.49529603 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.459\n",
      "INFO:tensorflow:step = 901, loss = 0.49428514 (0.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.151\n",
      "INFO:tensorflow:step = 1001, loss = 0.54822904 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.0586\n",
      "INFO:tensorflow:step = 1101, loss = 0.621345 (1.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.534\n",
      "INFO:tensorflow:step = 1201, loss = 0.7255604 (0.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.529\n",
      "INFO:tensorflow:step = 1301, loss = 0.52333814 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.161\n",
      "INFO:tensorflow:step = 1401, loss = 0.7117647 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.042\n",
      "INFO:tensorflow:step = 1501, loss = 0.2726413 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.13\n",
      "INFO:tensorflow:step = 1601, loss = 0.39237788 (0.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.109\n",
      "INFO:tensorflow:step = 1701, loss = 0.42655617 (0.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.815\n",
      "INFO:tensorflow:step = 1801, loss = 0.46713108 (0.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.662\n",
      "INFO:tensorflow:step = 1901, loss = 0.8118668 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.34\n",
      "INFO:tensorflow:step = 2001, loss = 0.51364475 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.725\n",
      "INFO:tensorflow:step = 2101, loss = 0.28075132 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.43\n",
      "INFO:tensorflow:step = 2201, loss = 0.10198349 (0.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.807\n",
      "INFO:tensorflow:step = 2301, loss = 0.7051683 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.84\n",
      "INFO:tensorflow:step = 2401, loss = 0.37629074 (0.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.384\n",
      "INFO:tensorflow:step = 2501, loss = 0.3350697 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.709\n",
      "INFO:tensorflow:step = 2601, loss = 0.23922227 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.718\n",
      "INFO:tensorflow:step = 2701, loss = 0.33412337 (0.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.49\n",
      "INFO:tensorflow:step = 2801, loss = 0.09483354 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.809\n",
      "INFO:tensorflow:step = 2901, loss = 0.39257196 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.804\n",
      "INFO:tensorflow:step = 3001, loss = 0.21097086 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.696\n",
      "INFO:tensorflow:step = 3101, loss = 0.21514875 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.228\n",
      "INFO:tensorflow:step = 3201, loss = 0.4903924 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.064\n",
      "INFO:tensorflow:step = 3301, loss = 0.31620407 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.235\n",
      "INFO:tensorflow:step = 3401, loss = 0.099412434 (0.636 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.619\n",
      "INFO:tensorflow:step = 3501, loss = 0.37015972 (0.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.95\n",
      "INFO:tensorflow:step = 3601, loss = 0.29038888 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.042\n",
      "INFO:tensorflow:step = 3701, loss = 0.08417974 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.786\n",
      "INFO:tensorflow:step = 3801, loss = 0.13685517 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.207\n",
      "INFO:tensorflow:step = 3901, loss = 0.15908505 (0.388 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /tmp/tmpow_e53kq/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.12927864.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpow_e53kq/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.914"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "y = y.astype(np.int)\n",
    "\n",
    "# MNIST dataset is actually already split into training(first 60,000 images) and test set (10,000 images)\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] \n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_cols)\n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=4000)\n",
    "\n",
    "y_pred = list(dnn_clf.predict(X_test))\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a DNN using plain TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(input_X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(input_X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(input_X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "        \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation = tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Neural Network using TensorFlow Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn_tf\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-91-1b9574c33fe9>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/dimension/.virtualenvs/data-science/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/dimension/.virtualenvs/data-science/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/dimension/.virtualenvs/data-science/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/dimension/.virtualenvs/data-science/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/dimension/.virtualenvs/data-science/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Train Accuracy:  0.92  Test Accuracy:  0.9144\n",
      "Epoch:  1  Train Accuracy:  0.96  Test Accuracy:  0.9329\n",
      "Epoch:  2  Train Accuracy:  0.94  Test Accuracy:  0.9395\n",
      "Epoch:  3  Train Accuracy:  0.9  Test Accuracy:  0.9464\n",
      "Epoch:  4  Train Accuracy:  0.98  Test Accuracy:  0.9504\n",
      "Epoch:  5  Train Accuracy:  0.96  Test Accuracy:  0.9553\n",
      "Epoch:  6  Train Accuracy:  0.98  Test Accuracy:  0.9582\n",
      "Epoch:  7  Train Accuracy:  0.94  Test Accuracy:  0.9602\n",
      "Epoch:  8  Train Accuracy:  0.96  Test Accuracy:  0.9629\n",
      "Epoch:  9  Train Accuracy:  0.98  Test Accuracy:  0.9637\n",
      "Epoch:  10  Train Accuracy:  0.98  Test Accuracy:  0.9646\n",
      "Epoch:  11  Train Accuracy:  0.98  Test Accuracy:  0.966\n",
      "Epoch:  12  Train Accuracy:  0.96  Test Accuracy:  0.9674\n",
      "Epoch:  13  Train Accuracy:  0.98  Test Accuracy:  0.9686\n",
      "Epoch:  14  Train Accuracy:  0.98  Test Accuracy:  0.9693\n",
      "Epoch:  15  Train Accuracy:  0.98  Test Accuracy:  0.9705\n",
      "Epoch:  16  Train Accuracy:  1.0  Test Accuracy:  0.9701\n",
      "Epoch:  17  Train Accuracy:  1.0  Test Accuracy:  0.9721\n",
      "Epoch:  18  Train Accuracy:  1.0  Test Accuracy:  0.9718\n",
      "Epoch:  19  Train Accuracy:  0.98  Test Accuracy:  0.9723\n",
      "Epoch:  20  Train Accuracy:  1.0  Test Accuracy:  0.973\n",
      "Epoch:  21  Train Accuracy:  0.96  Test Accuracy:  0.9731\n",
      "Epoch:  22  Train Accuracy:  1.0  Test Accuracy:  0.9735\n",
      "Epoch:  23  Train Accuracy:  0.98  Test Accuracy:  0.9743\n",
      "Epoch:  24  Train Accuracy:  1.0  Test Accuracy:  0.9752\n",
      "Epoch:  25  Train Accuracy:  1.0  Test Accuracy:  0.9749\n",
      "Epoch:  26  Train Accuracy:  0.98  Test Accuracy:  0.9755\n",
      "Epoch:  27  Train Accuracy:  1.0  Test Accuracy:  0.9755\n",
      "Epoch:  28  Train Accuracy:  0.98  Test Accuracy:  0.975\n",
      "Epoch:  29  Train Accuracy:  1.0  Test Accuracy:  0.9755\n",
      "Epoch:  30  Train Accuracy:  1.0  Test Accuracy:  0.9763\n",
      "Epoch:  31  Train Accuracy:  0.98  Test Accuracy:  0.9774\n",
      "Epoch:  32  Train Accuracy:  0.98  Test Accuracy:  0.9771\n",
      "Epoch:  33  Train Accuracy:  0.98  Test Accuracy:  0.9767\n",
      "Epoch:  34  Train Accuracy:  1.0  Test Accuracy:  0.9768\n",
      "Epoch:  35  Train Accuracy:  1.0  Test Accuracy:  0.9781\n",
      "Epoch:  36  Train Accuracy:  1.0  Test Accuracy:  0.9771\n",
      "Epoch:  37  Train Accuracy:  1.0  Test Accuracy:  0.9786\n",
      "Epoch:  38  Train Accuracy:  1.0  Test Accuracy:  0.9794\n",
      "Epoch:  39  Train Accuracy:  1.0  Test Accuracy:  0.9791\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iterations in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            \n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X:mnist.test.images, y:mnist.test.labels})\n",
    "        \n",
    "        print(\"Epoch: \", epoch, \" Train Accuracy: \", acc_train, \" Test Accuracy: \", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND, OR, NOR & NAND Gate Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[-2. -3.]]\n",
      "[4.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data for AND Gate\n",
    "# arr1, target = [[0, 0], [0, 1], [1, 0], [1,1]], [0, 0, 0, 1]\n",
    "\n",
    "# Data for OR Gate\n",
    "# arr1, target = [[0, 0], [0, 1], [1, 0], [1,1]], [0, 1, 1, 1]\n",
    "\n",
    "# Data for NOR Gate\n",
    "# arr1, target = [[0, 0], [0, 1], [1, 0], [1,1]], [1, 0, 0, 0]\n",
    "\n",
    "# Data for NAND Gate\n",
    "arr1, target = [[0, 0], [0, 1], [1, 0], [1,1]], [1, 1, 1, 0]\n",
    "\n",
    "X = np.array(arr1)\n",
    "y = np.array(target).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(random_state=42, max_iter=100)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict(X)\n",
    "\n",
    "print(accuracy_score(y, y_pred))\n",
    "\n",
    "print(per_clf.coef_)\n",
    "print(per_clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR Gate Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Accuracy:  0.5  Training Loss:  0.73084766  Prediction:  [0, 0, 0, 0]\n",
      "Epoch:  10  Training Accuracy:  0.25  Training Loss:  0.69431645  Prediction:  [1, 0, 0, 0]\n",
      "Epoch:  20  Training Accuracy:  0.75  Training Loss:  0.67686635  Prediction:  [1, 1, 1, 0]\n",
      "Epoch:  30  Training Accuracy:  0.75  Training Loss:  0.66742486  Prediction:  [1, 1, 1, 0]\n",
      "Epoch:  40  Training Accuracy:  0.75  Training Loss:  0.65644586  Prediction:  [1, 1, 1, 0]\n",
      "Epoch:  50  Training Accuracy:  0.75  Training Loss:  0.6445052  Prediction:  [1, 1, 1, 0]\n",
      "Epoch:  60  Training Accuracy:  0.75  Training Loss:  0.63128364  Prediction:  [1, 1, 1, 0]\n",
      "Epoch:  70  Training Accuracy:  0.75  Training Loss:  0.61573714  Prediction:  [1, 1, 1, 0]\n",
      "Epoch:  80  Training Accuracy:  0.5  Training Loss:  0.5979193  Prediction:  [1, 0, 1, 0]\n",
      "Epoch:  90  Training Accuracy:  0.5  Training Loss:  0.5775427  Prediction:  [1, 0, 1, 0]\n",
      "Epoch:  100  Training Accuracy:  0.5  Training Loss:  0.5558113  Prediction:  [1, 0, 1, 0]\n",
      "Epoch:  110  Training Accuracy:  0.5  Training Loss:  0.53318506  Prediction:  [1, 0, 1, 0]\n",
      "Epoch:  120  Training Accuracy:  0.75  Training Loss:  0.5142179  Prediction:  [1, 1, 1, 0]\n",
      "Epoch:  130  Training Accuracy:  0.5  Training Loss:  0.49628752  Prediction:  [1, 0, 1, 0]\n",
      "Epoch:  140  Training Accuracy:  0.5  Training Loss:  0.48042846  Prediction:  [1, 0, 1, 0]\n",
      "Epoch:  150  Training Accuracy:  0.75  Training Loss:  0.46524528  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  160  Training Accuracy:  0.75  Training Loss:  0.4516782  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  170  Training Accuracy:  0.75  Training Loss:  0.43968147  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  180  Training Accuracy:  0.75  Training Loss:  0.42875585  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  190  Training Accuracy:  0.75  Training Loss:  0.41861996  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  200  Training Accuracy:  0.75  Training Loss:  0.40972772  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  210  Training Accuracy:  0.75  Training Loss:  0.40160307  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  220  Training Accuracy:  0.75  Training Loss:  0.3931674  Prediction:  [0, 0, 1, 0]\n",
      "Epoch:  222  Final Accuracy:  1.0  Final Loss:  0.39136606  Final Prediction:  [0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 1000\n",
    "n_inputs = 2\n",
    "n_hidden1 = 5\n",
    "n_outputs = 2\n",
    "\n",
    "arr1, target = [[0, 0], [0, 1], [1, 0], [1,1]], [0, 1, 1, 0]\n",
    "\n",
    "X_data = np.array(arr1).astype(np.float32)\n",
    "y_data = np.array(target).astype(np.int)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn_tf\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):        \n",
    "        #sess.run(training_op, feed_dict={X:X_data, y:y_data})    \n",
    "        #acc_train = accuracy.eval(feed_dict={X:X_data, y:y_data})\n",
    "        feed_dict={X:X_data, y:y_data}\n",
    "        t_op, acc_train, t_los, corr = sess.run([training_op, accuracy, loss, correct], feed_dict)\n",
    "        prediction = [1 if x==y else 0 for (x,y) in zip(corr, target)]\n",
    "        if acc_train == 1.0: break\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: \", epoch, \" Training Accuracy: \", acc_train, \" Training Loss: \", t_los, \n",
    "                 \" Prediction: \", prediction)\n",
    "        \n",
    "print(\"Epoch: \", epoch, \" Final Accuracy: \", acc_train, \" Final Loss: \", t_los, \" Final Prediction: \", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
